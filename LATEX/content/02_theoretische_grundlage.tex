\chapter{Theoretische Grundlagen}

\section{Gammaastronomie}

Im Universum gibt es zahlreiche Prozesse, bei denen hochenergetische Teilchen entstehen, oder auf diese Energien beschleunigt werden.
Bei diesen Teilchen handelt es sich zum großen Teil um Protonen oder leichten  Atomkernen bis hin zu Eisen, aber auch Elektronen oder Myonen
sind Bestandteil der Kosmischen Strahlung.
Der Beschleunigungsprozess der Teilchen geschieht durch Druckwellen, wie sie bei Sternexplosionen vorkommen.
Dieser Prozess wird durch das Modell der Fermi-Beschleunigung erster und zweiter Ordnung beschrieben.
Bei der Fermi-Beschleunigung wird das Medium, indem die Druckwelle propagiert, durch ein Plasma beschrieben, welches Magnetfeldstörungen mit sich führt.
Wenn ein geladenes Teilchen auf eine solche Störung, welche sich mit einer Geschwindigkeit $v$ durch das Medium bewegt, trifft, wird es durch die
Lorentzkraft mit einem Winkel $\theta$ elastisch gestreut, wodurch es beschleuingt wird.
Wenn nun alle möglichen Winkel berücksichtigt werden, ergibt sich eine Energiegewinn von
\begin{equation*}
  \left\langle \frac{\delta E}{E} \right\rangle = \frac{8}{2}\left(\frac{v}{c}\right)^2\text{ .}
\end{equation*}
Bei einer typischen Druckwellengeschwindigkeit von $v=\SI{e4}{\m\per\s}$\cite[14]{HESS} ergibt sich ein Energiegewinn von $\SI{4.5e-9}{\m\per\s}$.
Dies ist die Fermibeschleunigung zweiter Art und kann die große Beschleunigung in Supernovae nicht alleine erklären.
Ein größeren Beitrag liefert die Fermibeschleunigung erster Art, bei der die Teilchen durch mehrfaches durchqueren der Schockfront beschleunigt werden.
Der Energiegewinn beträgt für alle Streuwinkel
\begin{equation*}
  \left\langle \frac{\delta E}{E} \right\rangle \approx \frac{2}{3}\frac{\delta v}{c} \text{ ,}
\end{equation*}
wobei $\delta v$ der Geschwindigkeitsunterschied zwischen der Materie hinter und vor der Schockwelle ist.

Für den Teilchenfluss der Kosmischen Strahlung nach der Fermibeschleunigung in Schockfronten von zum Beispiel Supernova Überresten(SR) oder Aktiven
Galaxie Kernen(AGN) gilt $\Phi(E)\approx E^{-2}$. Wenn die Strahlung durch das extragalaktische Plasma wird das Spektrum um $E^{-\frac{1}{3}}$ steiler und
da es das Plasma der Milchstraße durchqueren muss, damit es im Sonnensystem gemessen zu werden kann. Ist der Teilchenfluss der Kosmischen Strahlung, welche
durch die Fermibeschleunigung entsteht proportional zu $E^{-2.7}$\cite[5]{Cosmic_rays}.

Jedoch erklären diese Prozesse nicht den gemessenen Energiefluss von ultra hochenergetischen Teilchen. Bei Energien von $\SI{3e15}{\eV}$ tritt ein erstes
"knee" im Spektrum auf, welches nicht durch Fermibeschleunigung zu erklären ist.
Die Phänomene die Materie bis auf diese Energien beschleunigen, sind noch nicht vollständig erforscht, es wird zum Beispiel nach möglichen elektrischen
Potentialunterschieden gesucht, die Teilchen beschleunigen könnten.
Ein weiterer möglicher Prozess ist der Zerfall von Dunkler Materie oder im speziellen von Axionen, der vermutlich hochenergetische Teilchen erzeugt.

Diese hochenergetische Teilchenstrahlung erzeugt durch Wechselwirkungen oder Zerfälle Gammastrahlung.
Wichtige Prozesse bei der Erzeugung hochenergetischer Photonen sind die Wechselwirkungen von Photonen und Elektronen.
Die Elektron-Photon Wechelwirkung wird beschrieben durch die
Compton-Streuung, wobei ein Photon mit einem Elektron elastisch stößt, durch die inverse Compton-Streuung, wobei das geladene Teilchen Energie auf das Photon
überträgt, durch die Paarerzeugung, wobei aus einem Photon ein Elektron
und ein Positron enstehen, durch die Annihilation, welcher der Umkehrprozess zur Paarerzeugung ist und aus einem Elekron Positron Paar
ein Photon entsteht, und durch die Bremsstrahlung, welche bei der Impulsänderung gelandener Teilchen entsteht.

Diese hochenergetischen Photonen oder auch Gammastrahlung genannt, können entweder direkt mithilfe von Satelliten im Weltraum oder indirekt über das in
der Atmosphäre entstehende Schauer mithilfe von Cherenkov Teleskopen auf der Erdoberfläche beobachtet werden.
Die Gammastrahlung ist so energiereich, dass die Satelliten nur mithilfe von Szinillationszählern diese messen können und nicht durch Spiegelteleskope,
da diese hohen frequenzen nicht gespiegelt werden, sondern durch die Spiegeloberfläche transmittieren. Die Szintilationszähler haben jedoch eine nicht
so hohe Richtungsauflösung wie Spiegelteleskope.
Diese energiereiche Strahlung sorgt jedoch dafür, dass in der Erdatmosphäre durch die Wechselwirkung mit den Luftmolekülen ein hochrelativistisches
Teilchenschauer ensteht.
In diesem Schauer entstehen geladene Teilchen, die eine Geschwindigkeit besitzen, die über der Lichtgeschwindigkeit in Luft liegt.
Geladene Teilchen polarisieren die Luftmoleküle für kurze Zeit, wodurch sie elektromagnetische Wellen aussenden.
Wenn sich nun das geladene Teilchen schneller als die Lichtgeschwindigkeit des Mediums bewegt, können sich die Wellen nicht mehr destruktiv
überlagern und es enstehen Cherenkov-Blitze, welches sich kegelförmig ausbreitet und von den Teleskopen am Boden beobachtet werden kann.
Das Spektrum der Cherenkov-Strahlung ist kontinuierlich, die Intensität pro Frequenz ist jedoch proportional zur Frequenz im sichtbaren Bereich und
daher wird das Cherenkov-Licht als bläulich wahrgenommen. Da jedoch sowohl hochenergetische Gammastrahlung als auch hochenergetische Teilchenstrahlung
Schauer in der Atmosphäre erzeugen, gibt es einen großen Untergrund, der seperiert werden muss.

\section{Cherenkov Teleskope Array (CTA)}

Das Cherenkov Teleskop Array ist ein Zukunftsprojekt einer internationalen Kollaboration von 210 Instituten aus 32 Ländern\cite{CTA_consortium}
und der nächste Schritt in der hochenergie Gammastronomie.
Mit einer Gesamtanzahl von 108 Teleskopen hat das Array nach Simulationen zur Folge in seinem Hauptenergiebereich eine Sensitivität von $\SI{0.1}{\percent}$
des Energieflusses des Krebsnebels, was ungefähr zehn mal sensitiver als das HESS-Experiment ist\cite{CTA_paper}.
Da der Teilchenfluss $\Phi$ der kosmischen Strahlung in dem Sensitivitätsbereich von CTA dem Potenzgesetz $\Phi \propto E^{-2.7}$\cite[5]{Cosmic_rays} folgt,
treffen bei einer Energie $E$ von $\SI{1}{\tera\eV}$ noch $\SI{1}{\per\m\squared\per\s}$ auf die Erdatmosphäre.
Um dennoch genug Statistik zu haben, muss eine möglichst große Himmelfläche observiert werden.
CTA kann durch die große Anzahl an Teleskopen bei einer Beobachtungszeit von $\SI{0.5}{\hour}$ Photonen mit einer Energie von $\SI{1}{\tera\eV}$ auf einer
Fläche von $\SI{10e6}{\m\squared}$\cite{CTA_ob} beobachten.
Durch 3 verschiedene Größen von Teleskopen, das LST (Large Sized Telescope) mit einer Spiegelgröße von $\SI{23}{\m}$, das MST (Medium Sized Telescope)
mit einer Größe von $\SI{11.5}{\m}$ oder $\SI{9.7}{\m}$ und das SST (Small-Sized Telescope), welches eine Größe von $\SI{4.3}{\m}$ oder $\SI{4.0}{\m}$
besitzt, kann CTA Photonen mit Energien von $\SI{30}{\giga\eV}$ bis $\SI{300}{\tera\eV}$ detektieren.
Dieses breite Spektrum ist wichtig, um die ganze Bandbreite der Beschleunigungsprozesse im Universum untersuchen zu können.
Insbesondere kann das "knee" des Energiespektrums bei $\SI{3e15}{\eV}$ genau untersucht werden.
Die hohe Sensitivität und die niedrige Energieuntergrenze ermöglichen die Entdeckung neuer Quellen mit einer starken Rotverschiebung, die nur bei niedrigen
Energien sichtbar sind, da die höher energetische Gammastrahlung mit der Hintergundstrahlung wechselwirkt und somit nicht beobachtbar ist.

\section{Maschinelles Lernen}

Da die Photonen, die durch unverstandene Prozesse entstehen, einen Geringen Teilchenfluss besitzen, muss die Anzahl an beobachteten Ereignissen bei modernen
Experimenten stark ansteigen.
Eine händische Analyse ist nicht mehr möglich, daher werden Algorithmen trainiert,
die diese Aufgabe übernehmen. Diese Algorithmen müssen jedoch intelligente Entscheidungen eigenständig treffen können, wobei dann von künstliche Intelligenz
gesprochen werden kann.
Ein Teilgebiet der künstlichen Intelligenz ist das maschinelle Lernen(ML). Hierbei lernen Algorithmen aus Datensätzen, indem sie verschiedene Optimierungsverfahren
der Mathematik nutzen, um eine Fehlerfunktion zu minimieren. Ein trainierter Algorithmus ist in der Lage Vorhersagen für neue Datenpunkte zu treffen.

Der Bereich des maschinellen Lernens wird in zwei Bereiche gegliedert. Es gibt das überwachte Lernen, bei dem das Ergebnis und die Eingangsdaten bekannt
sind und es gibt das unüberwachte Lernen, bei dem der Algorithmus nur die Eingangsdaten kennt und in diesen Daten Muster sucht.
Zwei große Aufgabengebiete im Bereich des überwachten Lernens sind die Regression und die Klassifikation. Bei der Regression ist der Ausgabewert eine
kontinuierliche Variable und bei der Klassifikation ist das Ergebnis in $N$ Klassen eingeteilt. Die Regression kann somit als Grenzfall
$N \to \infty$ der Klassifikation verstanden werden.

Das Modell der Regressionsanalyse ist gegeben durch die abhängige Variable $y$ die über eine Funktion $f(X,\theta)$ von der Variable $X$ abhängt.
Das Ziel der Analyse ist es, den Parameter $\theta$ so zu optimieren, dass $\hat{y} = f(X,\theta) + L(\hat{y},y)$ mit einem minimierten Fehler
$L(\hat{y},y)$ gilt.
Wenn $\vec{\theta}$ ein Vektor aus k Parametern ist und $(X_i,y_i)$ $N$ Tupel sind, müssen drei Fälle unterschieden werden.
Im ersten Fall ist $k>N$ und somit das System unterbestimmt.
Für unterbestimmte Syteme gibt es nicht genug Datenpunkte, um alle Parameter vorherzusagen und somit führen viele Regressionsmethoden
nicht zum Ziel.
Bei $k = N$ existiert genug Information um ein lineares System exakt zu lösen.
Im letzten Fall $k<N$ ist das System überbestimmt und es gibt mehrere Lösungen, wobei die Lösung gewählt wird, die den Fehler minimiert.
Damit die Regressionsanalyse funktioniert, muss der Trainingsdatensatz das Problem vollständig repräsentieren. Außerdem darf der Fehler $L(\hat{y},y)$ keinen Trend
oder Korrelation besitzen.
Eine weitere Annahmen über das Problem muss sein, dass die Variable $X$ unkorreliert und linear unabhängig ist.
Wenn ein Problem eine unkonstante Varianz des Fehlers besitzt, muss dies durch eine gewichtete Methode korrigiert werden.

\section{Random Forest Regressor}

Eine Methode des überwachten Lernens, welche für die Regression verwendet werden kann, ist der Random Forest Algorithmus. Dieser Algorithmus baut
einen Wald aus mehreren unkorrelierten Entscheidungsbäumen auf, die eigenständige Vorhersagen treffen, über die gemittelt werden kann.

Ein Entscheidungsbaum ist so aufgebaut, dass zu Begin der gesamte Datensatz, der sogenannten 'Stamm', aufgeteilt wird.
Dadurch entstehen "Blätter", dir nur noch aus einem Teil des Datensatzes bestehen.
Der Datensatz wird so getrennt, dass ein gewähltes Kriterium optimiert wird. Dieses Kriterium
kann die Gini Unreinheit oder der Informationsgewinn sein, bei der Regression wird jedoch häufig die Varianz Reduktion verwendet. Bei dieser Optimierung
wird in jedem Schritt der mittlere quadratische Fehler
\begin{equation}
  H(X_m) = \frac{1}{N_m}\sum_{i\in N_m}(y_i-c_m)^2
\end{equation}
jedes Teildatensatzes $m$ minimiert, mit
\begin{equation}
  c_m = \frac{1}{N_m}\sum_{i\in N_m}y_i
\end{equation}
als Mittelwert der Vorhersagen $y_i$ für jeden Datenpunkt $i$.
Dies wird rekursiv wiederholt und somit der Baum ausgebaut, bis ein Abbruchkriterium erfüllt ist. Dieses Abbruchkriterium kann eine vorher festgelegte maximale Tiefe
des Baumes sein, eine minimale Größe des Datensatzes der getrennt werden soll oder eine minimale Größe des getrennten Datensatzes. Nach erreichen der Abbruchbedigung ist
$c_m$ des letzten Blattes die endgültige Vorhersage.

Bei Entscheidungsbäumen gibt es eine Vielzahl von Umsetzungen, die aktuellsten Arten sind jedoch der C4.5 und der CART Algorithmus\cite[1]{CART}. Die Besonderheit am C5.0 Algorithmus,
das er im Gegensatz zum CART Algorithmus den Datensatz nicht nur binär trennen kann, jedoch kann mit ihm keine Regression durchgeführt werden. Im \textsc{scikit-learn}
Framework\cite{scikit-learn} von Python ist der CART-Algorithmus implementiert, der zur Regression genutzt werden kann und die Varianze Reduktion als Optimierungskriterium nutzt.

Die Vorteile eines Entscheidungsbaums sind die Interpretierbarkeit, der logarithmische Zusammenhang zwischen Trainingsdauer und Datengröße und die einfache Datenpreparation die
nur notwendig ist. Außerdem ist der Entscheidungsbaum nicht anfällig gegenüber unbedeutenen Attributen. Nachteile der Entscheidungsbäume ist die Gefahr des übertrainierens. Dies
führt dazu, dass durch zu großes Ausbauen des Baumes der Trainingsdatensatz zu gut nachgebildet wird und dadurch die Vorhersagen für einen unabhängigen Testdatensatz unpräziser sind.
Desweiteren kann es zu einem Trend in den Vorhersagen kommen, wenn die Statistik des Ergebnisses einen großen Trend aufweist.
Außerdem sind die Entscheidungsbäume nicht stabil, da leichte Änderungen im Datensatz zu grob abweichenden Entscheidungsbäumen führen, damit ist die Varainz bei Vorhersagen
unterschiedlicher Bäume sehr hoch.
Da der Entscheidungsbaum zu den gierigen Algorithmen gehört, welche die Entscheidung aufgrund des derzeitig besten Gewinn treffen, findet dieses Verfahren sehr schnell ein
Optimum, jedoch ist dieses Extremum nicht immer das globale Extremum und somit nicht die optimalste Lösung.
Die letzten beiden Nachteile können durch die Erweiterung des Entscheidungsbaum zu einem Entscheidungswald behoben werden.

Dieses Verfahren wird Random Forest(RF) genannt. Beim Random Forest werden eine Vielzahl von Entscheidungsbäume die, um Unkorreliertheit der Ergebnisse zu erreichen, mit
zufällig ausgewählten Teildatensätzen und Attribute trainiert werden. Das Ergebnis des RF ergibt sich durch eine Mittelung über alle Entscheidungsbäume.
Wenn die Anzahl der Entscheidungsbäume erhöht wird konvergiert generalisierte Fehler
\begin{equation}
  PE = P_{X,y}(mg(X,y)<0)
\end{equation}
gegen
\begin{equation}
  P_{X,y}(P_\theta(h(X,\theta)=y)-\max_{j\neq y}P_\theta(h(X,\theta)=j)<0)
\end{equation}
und somit kann es durch eine Vergrößerung des Waldes nicht zum Übertraining kommen[7]\cite{RandomForests_Breiman}. Bei diesem Theorem ist
\begin{equation}
  mg(X,y) = av_k I(h_k(X)=y) - \max_{j \neq y}av_k I(h_k(X)=j)
\end{equation}
die Gewinn Funktion, $h_k(X)$ die Vorhersage der einzelnen Entscheidungsbäume und $I(\cdot)$ die charakteristische Funktion, welche eine $1$ ergibt, wenn der
Entscheidungsbaum das geforderte Ergebnis liefert und eine $0$ wenn nicht. Wenn die Gewinn Funktion also über null liegt, sagt der Entscheidungsbaum das richtige Ergebnis vorher.

Um einen unkorrelierten Wald zu erhalten, muss die Auswahl der Teildatensätzen und der Attribute zufällig und unabhängig getroffen werden. Dazu kann unteranderem das Adaboost
Verfahren oder das einfachere Bagging verwendet werden. Das in \textsc{scikit-learn} verwendete Bagging funktioniert, indem aus dem Datensatz $N$ Stichporben der Größe
$M$ gezogen werden und für die $N$ Entscheidungsbäume verwendet werden. Die $N$ Ergebnisse werden am Ende gemittelt oder mit der Genauigkeit des jeweiligen Ergebnisses gewichtet
gemittelt. Auch die Attribute werden in einem Umfang, der festgelegt werden kann, zufällig gezogen. Hierdurch wird der Trend der einzelnen Ergebnisse größer jedoch durch die
Mittelung wird die Standardabweichung der Ergebnisse ungleich geringer und somit führt Bagging zu einem besseren Ergebnis.


% Breiman RF Theorem 1.2
%Randomforest
%bagging
\section{Energie Rekonstruktion}
%Hillas Parameter
%Datenstruktur und Level
%erfüllt es die Annahmen für eine Regression

\section{Mean Scaled Value}


\section{Performance}
