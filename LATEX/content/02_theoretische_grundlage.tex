\chapter{Theoretische Grundlagen}

\section{Gammaastronomie}

Im Universum gibt es zahlreiche Prozesse, bei denen hochenergetische Teilchen entstehen, oder auf diese Energien beschleunigt werden.
Bei diesen Teilchen handelt es sich zum großen Teil um Protonen oder leichten  Atomkernen bis hin zu Eisen, aber auch Elektronen oder Myonen
sind Bestandteil der Kosmischen Strahlung.
Der Beschleunigungsprozess der Teilchen geschieht durch Druckwellen, wie sie bei Sternexplosionen vorkommen.
Dieser Prozess wird durch das Modell der Fermi-Beschleunigung erster und zweiter Ordnung beschrieben.
Bei der Fermi-Beschleunigung wird das Medium, indem die Druckwelle propagiert, durch ein Plasma beschrieben, welches Magnetfeldstörungen mit sich führt.
Wenn ein geladenes Teilchen auf eine solche Störung, welche sich mit einer Geschwindigkeit $v$ durch das Medium bewegt, trifft, wird es durch die
Lorentzkraft mit einem Winkel $\theta$ elastisch gestreut, wodurch es beschleuingt wird.
Wenn nun alle möglichen Winkel berücksichtigt werden, ergibt sich eine Energiegewinn von
\begin{equation*}
  \left\langle \frac{\delta E}{E} \right\rangle = \frac{8}{2}\left(\frac{v}{c}\right)^2\text{ .}
\end{equation*}
Bei einer typischen Druckwellengeschwindigkeit von $v=\SI{e4}{\m\per\s}$\cite[14]{HESS} ergibt sich ein Energiegewinn von $\SI{4.5e-9}{\m\per\s}$.
Dies ist die Fermibeschleunigung zweiter Art und kann die große Beschleunigung in Supernovae nicht alleine erklären.
Ein größeren Beitrag liefert die Fermibeschleunigung erster Art, bei der die Teilchen durch mehrfaches durchqueren der Schockfront beschleunigt werden.
Der Energiegewinn beträgt für alle Streuwinkel
\begin{equation*}
  \left\langle \frac{\delta E}{E} \right\rangle \approx \frac{2}{3}\frac{\delta v}{c} \text{ ,}
\end{equation*}
wobei $\delta v$ der Geschwindigkeitsunterschied zwischen der Materie hinter und vor der Schockwelle ist.

Für den Teilchenfluss der Kosmischen Strahlung nach der Fermibeschleunigung in Schockfronten von zum Beispiel Supernova Überresten(SR) oder Aktiven
Galaxie Kernen(AGN) gilt $\Phi(E)\approx E^{-2}$. Wenn die Strahlung durch das extragalaktische Plasma wird das Spektrum um $E^{-\frac{1}{3}}$ steiler und
da es das Plasma der Milchstraße durchqueren muss, damit es im Sonnensystem gemessen zu werden kann. Ist der Teilchenfluss der Kosmischen Strahlung, welche
durch die Fermibeschleunigung entsteht proportional zu $E^{-2.7}$\cite[5]{Cosmic_rays}.

Jedoch erklären diese Prozesse nicht den gemessenen Energiefluss von ultra hochenergetischen Teilchen. Bei Energien von $\SI{3e15}{\eV}$ tritt ein erstes
"knee" im Spektrum auf, welches nicht durch Fermibeschleunigung zu erklären ist.
Die Phänomene die Materie bis auf diese Energien beschleunigen, sind noch nicht vollständig erforscht, es wird zum Beispiel nach möglichen elektrischen
Potentialunterschieden gesucht, die Teilchen beschleunigen könnten.
Ein weiterer möglicher Prozess ist der Zerfall von Dunkler Materie oder im speziellen von Axionen, der vermutlich hochenergetische Teilchen erzeugt.

Diese hochenergetische Teilchenstrahlung erzeugt durch Wechselwirkungen oder Zerfälle Gammastrahlung.
Wichtige Prozesse bei der Erzeugung hochenergetischer Photonen sind die Wechselwirkungen von Photonen und Elektronen.
Die Elektron-Photon Wechelwirkung wird beschrieben durch die
Compton-Streuung, wobei ein Photon mit einem Elektron elastisch stößt, durch die inverse Compton-Streuung, wobei das geladene Teilchen Energie auf das Photon
überträgt, durch die Paarerzeugung, wobei aus einem Photon ein Elektron
und ein Positron enstehen, durch die Annihilation, welcher der Umkehrprozess zur Paarerzeugung ist und aus einem Elekron Positron Paar
ein Photon entsteht, und durch die Bremsstrahlung, welche bei der Impulsänderung gelandener Teilchen entsteht.

Diese hochenergetischen Photonen oder auch Gammastrahlung genannt, können entweder direkt mithilfe von Satelliten im Weltraum oder indirekt über das in
der Atmosphäre entstehende Schauer mithilfe von Cherenkov Teleskopen auf der Erdoberfläche beobachtet werden.
Die Gammastrahlung ist so energiereich, dass die Satelliten nur mithilfe von Szinillationszählern diese messen können und nicht durch Spiegelteleskope,
da diese hohen frequenzen nicht gespiegelt werden, sondern durch die Spiegeloberfläche transmittieren. Die Szintilationszähler haben jedoch eine nicht
so hohe Richtungsauflösung wie Spiegelteleskope.
Diese energiereiche Strahlung sorgt jedoch dafür, dass in der Erdatmosphäre durch die Wechselwirkung mit den Luftmolekülen ein hochrelativistisches
Teilchenschauer ensteht.
In diesem Schauer entstehen geladene Teilchen, die eine Geschwindigkeit besitzen, die über der Lichtgeschwindigkeit in Luft liegt.
Geladene Teilchen polarisieren die Luftmoleküle für kurze Zeit, wodurch sie elektromagnetische Wellen aussenden.
Wenn sich nun das geladene Teilchen schneller als die Lichtgeschwindigkeit des Mediums bewegt, können sich die Wellen nicht mehr destruktiv
überlagern und es enstehen Cherenkov-Blitze, welches sich kegelförmig ausbreitet und von den Teleskopen am Boden beobachtet werden kann.
Das Spektrum der Cherenkov-Strahlung ist kontinuierlich, die Intensität pro Frequenz ist jedoch proportional zur Frequenz im sichtbaren Bereich und
daher wird das Cherenkov-Licht als bläulich wahrgenommen. Da jedoch sowohl hochenergetische Gammastrahlung als auch hochenergetische Teilchenstrahlung
Schauer in der Atmosphäre erzeugen, gibt es einen großen Untergrund, der seperiert werden muss.

\section{Cherenkov Teleskope Array (CTA)}

Das Cherenkov Teleskop Array ist ein Zukunftsprojekt einer internationalen Kollaboration von 210 Instituten aus 32 Ländern\cite{CTA_consortium}
und der nächste Schritt in der hochenergie Gammastronomie.
Mit einer Gesamtanzahl von 108 Teleskopen hat das Array nach Simulationen zur Folge in seinem Hauptenergiebereich eine Sensitivität von $\SI{0.1}{\percent}$
des Energieflusses des Krebsnebels, was ungefähr zehn mal sensitiver als das HESS-Experiment ist\cite{CTA_paper}.
Da der Teilchenfluss $\Phi$ der kosmischen Strahlung in dem Sensitivitätsbereich von CTA dem Potenzgesetz $\Phi \propto E^{-2.7}$\cite[5]{Cosmic_rays} folgt,
treffen bei einer Energie $E$ von $\SI{1}{\tera\eV}$ noch $\SI{1}{\per\m\squared\per\s}$ auf die Erdatmosphäre.
Um dennoch genug Statistik zu haben, muss eine möglichst große Himmelfläche observiert werden.
CTA kann durch die große Anzahl an Teleskopen bei einer Beobachtungszeit von $\SI{0.5}{\hour}$ Photonen mit einer Energie von $\SI{1}{\tera\eV}$ auf einer
Fläche von $\SI{10e6}{\m\squared}$\cite{CTA_ob} beobachten.
Durch 3 verschiedene Größen von Teleskopen, das LST (Large Sized Telescope) mit einer Spiegelgröße von $\SI{23}{\m}$, das MST (Medium Sized Telescope)
mit einer Größe von $\SI{11.5}{\m}$ oder $\SI{9.7}{\m}$ und das SST (Small-Sized Telescope), welches eine Größe von $\SI{4.3}{\m}$ oder $\SI{4.0}{\m}$
besitzt, kann CTA Photonen mit Energien von $\SI{30}{\giga\eV}$ bis $\SI{300}{\tera\eV}$ detektieren.
Dieses breite Spektrum ist wichtig, um die ganze Bandbreite der Beschleunigungsprozesse im Universum untersuchen zu können.
Insbesondere kann das "knee" des Energiespektrums bei $\SI{3e15}{\eV}$ genau untersucht werden.
Die hohe Sensitivität und die niedrige Energieuntergrenze ermöglichen die Entdeckung neuer Quellen mit einer starken Rotverschiebung, die nur bei niedrigen
Energien sichtbar sind, da die höher energetische Gammastrahlung mit der Hintergundstrahlung wechselwirkt und somit nicht beobachtbar ist.

\section{Maschinelles Lernen}

Da die Photonen, die durch unverstandene Prozesse entstehen, einen Geringen Teilchenfluss besitzen, steigt die Anzahl an beobachteten Ereignissen bei modernen
Experimenten stark an.
Um diese Ereignisse händisch zu analysieren, ist die Anzahl der qualifizierten Arbeiter zu gering, daher werden Algorithmen trainiert,
die diese Aufgabe übernehmen. Diese Algorithmen müssen jedoch intelligente Entscheidungen eigenständig treffen können, womit von künstliche Intelligenz
gesprochen werden kann.
Ein Teilgebiet der künstlichen Intelligenz ist das maschinelle Lernen(ML). Hierbei lernen Algorithmen aus Datensätzen, indem sie verschiedene Optimierungsverfahren
der Mathematik benutzen, um eine Fehlerfunktion zu minimieren. Ein trainierter Algorithmus ist dann in der Lage Vorhersagen für neue Datenpunkte zu treffen.

Der Bereich des maschinellen Lernens wird in zwei Methoden gegliedert. Es gibt das überwachte Lernen, bei dem das Ergebnis und die Eingansdaten bekannt
sind und es gibt das unüberwachte Lernen, bei dem der Algorithmus nur die Eingansdaten kennt und in diesen Daten Muster bildet.
Zwei große Aufgabengebiete im Bereich des überwachten Lernens sind die Regression und die Klassifikation. Bei der Regression ist die Ausgabe eine
kontinuierliche Variable und bei der Klassifikation ist der Ausgabeparameter in $N$ Klassen eingeteilt. Die Regression kann somit als Grenzfall
$N \to \infty$ der Klassifikation verstanden werden.

Das Modell der Regressionsanalyse ist gegeben durch abhängige Variablen $y$ die über eine Funktion $f(X,\theta)$ von den Variablen $X$ abhängen.
Das Ziel der Analyse ist die Parameter $\theta$ so zu optimieren, dass $\hat{y} = f(X,\theta) + L(\hat{y},y)$ mit einem möglichst geringem Fehler
$L(\hat{y},y)$ gilt.
Wenn $\theta$ ein Vektor aus k Parametern ist und $(X,y)$ $N$ Tupel sind, müssen drei Fälle unterschieden werden.
Im ersten Fall ist $k>N$ und somit das System unterbestimmt.
In diesem Fall gibt es nicht genug Datenpunkte um alle Parameter vorher zusagen und somit kommen die meisten Regressionsmethoden
nicht zum Ziel.
Bei $k = N$ existiert genug Information um ein lineares System exakt zu lösen.
Im letzten Fall $k<N$ ist das System sogar überbestimmt und es gibt mehrere Lösungen, wobei die Lösung gewählt wird, die den Fehler minimiert.
Damit die Regressionsanalyse funktioniert, muss der Datensatz das Problem vollständig representieren, außerdem darf der Fehler $L(\hat{y},y)$ keinen Trend
haben und unkorreliert sein.
Eine weitere Annahmen über das Problem muss sein, dass die Variablen $X$ unkorreliert und linear unabhängig sind.
Ein Problem stellt eine unkonstante Varianz des Fehlers da, was jedoch durch eine gewichtete Methode korrigiert werden kann.

\section{Random Forest Regressor}

Eine Methode des überwachten Lernens, welche für die Regression verwendet werden kann, ist der Random Forest Algorithmus. Dieser Algorithmus baut
einen Wald aus mehreren unkorrelierten Entscheidungsbäumen, die somit eigenständige Vorhersagen liefern über die gemittelt werden kann.

Ein Entscheidungsbaum ist so aufgebaut, dass zu Begin der gesamte Datensatz in den sogenannten 'Stamm' gegeben wird. Dort wird der Datensatz aufgeteilt
und es entstehen Blätter mit einem Teil des Datensatzes. Der Datensatz wird so getrennt, dass ein gewähltes Kriterium optimiert wird. Dieses Kriterium
kann die Gini Unreinheit oder der Informationsgewinn sein, bei der Regression wird jedoch häufig die Variance Reduktion verwendet.

Dies wird rekursiv wiederholt, bis ein Abbruchkriterium erfüllt ist. Dieses Abbruchkriterium kann eine vorher festgelegte maximale Tiefe des Baumes sein,
eine minimale Größe des Datensatzes der getrennt werden soll oder eine minimale Größe des getrennten Datensatzes.
%Decissiontree
%Randomforest
%bagging
\section{Energie Rekonstruktion}
%Hillas Parameter
%Datenstruktur und Level
%erfüllt es die Annahmen für eine Regression

\section{Mean Scaled Value}


\section{Performance}
